<!DOCTYPE html>
<!-- saved from url=(0056)https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link rel="stylesheet" href="./resources/styles.css">
  <title>CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration</title>
  <meta name="description" content="SIGGRAPH Asia 2023">
  
  <meta property="og:image" content="http://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/figures/tilingnn.png">
  <meta property="og:title" content="CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration. In SIGGRAPH Asia 2023.">
  <script type="text/javascript">
    google.load("jquery", "1.3.2");
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <style type="text/css">
    body {
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
    }

    h1 {
    font-weight: 300;
    }

    .disclaimerbox {
      background-color: #eee;
      border: 1px solid #eeeeee;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
      padding: 20px;
    }

    video.header-vid {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.header-img {
      height: 140px;
      border: 1px solid black;
      border-radius: 10px;
      -moz-border-radius: 10px;
      -webkit-border-radius: 10px;
    }

    img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    margin-top: -35px;
    }

    a:link,
    a:visited {
      color: #1367a7;
      text-decoration: none;
    }

    a:hover {
      color: #208799;
    }

    td.dl-link {
      height: 160px;
      text-align: center;
      font-size: 22px;
    }

    .layered-paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35),
        /* The third layer shadow */
        15px 15px 0 0px #fff,
        /* The fourth layer */
        15px 15px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fourth layer shadow */
        20px 20px 0 0px #fff,
        /* The fifth layer */
        20px 20px 1px 1px rgba(0, 0, 0, 0.35),
        /* The fifth layer shadow */
        25px 25px 0 0px #fff,
        /* The fifth layer */
        25px 25px 1px 1px rgba(0, 0, 0, 0.35);
      /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }

    .paper-big {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35);
      /* The top layer shadow */
      margin-left: 10px;
      margin-right: 45px;
    }

    .layered-paper {
      /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
        0px 0px 1px 1px rgba(0, 0, 0, 0.35),
        /* The top layer shadow */
        5px 5px 0 0px #fff,
        /* The second layer */
        5px 5px 1px 1px rgba(0, 0, 0, 0.35),
        /* The second layer shadow */
        10px 10px 0 0px #fff,
        /* The third layer */
        10px 10px 1px 1px rgba(0, 0, 0, 0.35);
      /* The third layer shadow */
      margin-top: 5px;
      margin-left: 10px;
      margin-right: 30px;
      margin-bottom: 5px;
    }

    .vert-cent {
      position: relative;
      top: 50%;
      transform: translateY(-50%);
    }

    hr {
      border: 0;
      height: 1px;
      background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
  </style>
  <title></title>
</head>

<body data-gr-c-s-loaded="true" cz-shortcut-listen="true">
  <br>
  <center>
    <span style="font-size:34px">CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration    </span><br>
    <br>
    <table align="center" width="900px">
      <tbody>
        <tr>
          <td align="center" width="150px">
            <center>
              <span style="font-size:22px">Jingyu Hu*<sup>1</sup></a></span>
            </center>
          </td>
          <td align="center" width="150px">
            <center>
              <span style="font-size:22px"><a href="https://appsrv.cse.cuhk.edu.hk/~khhui/">Ka-Hei Hui*<sup>1</sup></span>
            </center>
          </td>
          <td align="center" width="200px">
            <center>
              <span style="font-size:22px"><a href="https://liuzhengzhe.github.io/">Zhengzhe Liu<sup>1</sup></a></span>
            </center>
          </td>
          <td align="center" width="200px">
            <center>
              <span style="font-size:22px"><a href="https://www.cs.sfu.ca/~haoz/">Hao Zhang<sup>2</sup></a></span>
            </center>
          </td>
          <td align="center" width="200px">
            <center>
              <span style="font-size:22px"><a href="http://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu<sup>1</sup></a></span>
            </center>
          </td>
          <td align="center" width="100">
            <center>
              <span style="font-size:15px">(* joint first authors)<sup>1</sup></a></span>
            </center>
          </td>
        </tr>
      </tbody>
    </table>
    <table align="center" width="800px">
      <tbody>
        <tr>
          <td align="center" width="50px"></td>
          <td align="center" width="400px">
            <center>
              <img class="img-fluid d-block logo" height="50px" src="./resources/cuhk_logo.png">
            </center>
          </td>
          <td align="center" width="300px">
            <center>
              <img class="img-fluid d-block logo" height="50px" src="./resources/sfu_logo.png">
            </center>
          </td>
          <td align="center" width="50px"></td>
        </tr>
      </tbody>
    </table>
    <table align="center" width="800px">
      <tbody>
        <tr>
          <td align="center" width="800px&quot;">
            <center></center>
          </td>
        </tr>
      </tbody>
    </table>
    <h3 class=""><span style="font-weight: normal;">SIGGRAPH Asia 2023 (Conference Track)</span></h3>
<!--	<h3 class=""><span style="font-weight: normal; margin-top: -10px;">ACM Transaction on Graphics (SIGGRAPH Asia) 38, 6 (2019)</span></h3>
-->
    <br>
    <table align="center" width="1100px">
      <tbody>
        <tr>
          <td align="center" width="275px">
            <center>
              <span style="font-size:18px"></span>
            </center>
          </td>
          <td align="center" width="275px">
            <center>
              <span style="font-size:18px"></span>
            </center>
          </td>
        </tr>
      </tbody>
    </table>
  </center>
  <br>
  <table align="center" width="1100px">

    <tbody>
      <tr>
        <td width="250">
          <center style="">
            <img class="rounded" src="./resources/teaser.png" width="1000px">
            <span class="description" align="center">
              CLIPXPlore demonstrates new capabilities of leveraging a vision-language model to explore the 3D shape space: binary-attribute-guided, text-guided, and sketch-guided.
              </span>
            <br>
          </center>
        </td>
      </tr>
      <tr>
        <td width="400px">
          <center style="">
            <img class="rounded" src="./resources/overview.png" width="1000px">
            <span class="description" align="center">
              Overview of CLIPXPlore. (a) summarizes our employed data. (b) connects the CLIP and shape spaces. (c) co-optimizes the CLIP code ùëê and its coupled shape code ùëß. (d) locates the latent direction associated with the given condition, allowing us to explore the shape space.
            </span>
            <br>
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <br>
  <hr>
  <center>
    <h1>Abstract</h1>
  </center>This paper presents CLIPXPlore, a new framework that leverages a vision-language model to guide the exploration of the 3D shape space. Many recentmethods have been developed to encode 3D shapes into a learned latentshape space to enable generative design and modeling. Yet, existing methods lack effective exploration mechanisms, despite the rich information. To this end, we propose to leverage CLIP, a powerful pre-trained vision-language model, to aid the shape-space exploration. Our idea is threefold. First, we couple the CLIP and shape spaces by generating paired CLIP and shape codes through sketch images and training a mapper network to con-nect the two spaces. Second, to explore the space around a given shape, we formulate a co-optimization strategy to search for the CLIP code that better matches the geometry of the shape. Third, we design three exploration modes, binary-attribute-guided, text-guided, and sketch-guided, to locate suitable exploration trajectories in shape space and induce meaningful changes to the shape. We perform a series of experiments to quantitatively and visually compare CLIPXPlore with different baselines in each of the three exploration modes, showing that CLIPXPlore can produce many meaningful exploration results that cannot be achieved by the existing solutions.<br><br>
  <hr>
  <br>
  <center>
  <h1>Results</h1>
</center>
<br>
  <table align="center" width="1100px">
    <tbody>
      <tr>
        <td width="400px">
          <center style="">
            <img class="rounded" src="./resources/anno_man.png" width="1000px">
            <span class="description" align="left">Results on binary-attribute-guided exploration. Our explored shapes are produced to match the binary attribute conditions; see the armrest growing from the chair and the drawer/shelf growing on the tables.</span>
            <br>
          </center>
        </td>
      </tr>
    </tbody>
    <tbody>
    <tr>
      <td width="400px">
        <center style="">
          <img class="rounded" src="./resources/text_man.png" width="750px">
          <span class="description" align="left">Results on text-guided exploration. By providing a diverse set of target shape descriptions, we can explore various input shapes to produce results that satisfy the corresponding description, e.g., the overall appearance of shapes (truck and jeep, short v.s. tall tables), and various topological structures (spindles/stretchers).</span>
          <br>
        </center>
      </td>
    </tr>
    </tbody>
    <tbody>
      <tr>
        <td width="400px">
          <center style="">
            <img class="rounded" src="./resources/sketch_man.png" width="750px">
            <span class="description" align="left">Results on sketch-guided exploration. Our framework can support changing the shape‚Äôs local properties, such as adding engines to the airplane and changing the top of the car. We can also add or remove some local structures, such as stretchers on tables</span>
            <br>
          </center>
        </td>
      </tr>
      </tbody>
  </table>
  <hr>
  <center>
    <h1>Paper and Supplementary Material</h1>
  </center>
  <table align="center" width="500" px="">
    <tbody>
      <tr>
        <td><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"></a></td>
        <td>
          <strong><span style="font-size:12pt">CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration</span></strong><br>
          <span style="font-size:12pt">In SIGGRAPH Asia 2023. <br>
        <a href="https://arxiv.org/abs/2306.08226">[Paper]</a>
<!--        <a href="https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/TilinGNN_present.mp4">[presentation]</a>-->
        <!--<a href="https://github.com/edward1997104/Wavelet-Generation">[code]</a>-->

      </span></td>
      </tr>
    </tbody>
  </table>
  <br><br>
  <hr>
<!--  <div class="py-5">-->
<!--    <div class="container">-->
<!--      <div class="row">-->
<!--        <div class="col-md-12">-->
<!--          <h1 class="" align="center">Video</h1>-->
<!--        </div>-->
<!--      </div>-->
<!--      <div class="container">-->
<!--        <div class="row">-->
<!--          <div class="col-md-12">-->
<!--            <iframe align="center" width="1120" height="630" src="https://www.youtube.com/embed/3WBNjEAqh98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--  <hr>-->
<!--  <div class="py-5">-->
<!--    <div class="container">-->
<!--      <div class="row">-->
<!--        <div class="col-md-12">-->
<!--          <h1 class="" align="center">Bibtex</h1>-->
<!--        </div>-->
<!--      </div>-->
<!--      <div class="row">-->
<!--        <div class="col-md-12">-->
<!--          <pre align="left">-->
<!--            @inproceedings{hui2022template,-->
<!--                title = {Neural Template: Topology-aware Reconstruction and Disentangled Generation of 3D Meshes},-->
<!--                author = {Ka-Hei Hui* and Ruihui Li and Jingyu Wu and Chi-Wing Fu(* joint first authors)},-->
<!--                booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},-->
<!--                year={2022}-->
<!--            }-->
<!--&lt;!&ndash;            To be updated.&ndash;&gt;-->
<!--          </pre>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--  <hr>-->
<table align="center" width="1100px">
    <tbody>
      <tr>
        <td width="400px">
          <left>
            <center>
              <h1>Acknowledgments</h1>
            </center>
            The authors thank the anonymous reviewers for their valuable
            comments. We also acknowledge the help from TANG Wai Lun for
            the data preparation. This work is supported by Shenzhen Portion
            of Shenzhen-Hong Kong Science and Technology Innovation Co-
            operation Zone (Project No. HZQB-KCZYB-20200089), Research
            Grants Council of the Hong Kong Special Administrative Region
            (Project no. CUHK 14206320 & 14201921), and Natural Sciences and
            Engineering Research Council of Canada (Project No. 611370). </left>
        </td>
      </tr>
    </tbody>
  </table>
  <br>
  <br>
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="col-md-12"></div>
      </div>
    </div>
  </div>
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="col-md-12"></div>
      </div>
    </div>
  </div>
	<hr>
  <div class="py-5">
    <div class="container">
      <div class="row">
        <div class="col-md-12"> </div>
      </div>
    </div>
  </div>
